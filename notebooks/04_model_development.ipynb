{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Predicting Antibody Binding from Amino Acid Sequences\n",
                "\n",
                "## Model Development\n",
                "\n",
                "This notebook focuses on developing and tuning machine learning models for predicting antibody binding from amino acid sequence features."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data processing\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Machine learning\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.svm import SVC\n",
                "import xgboost as xgb\n",
                "\n",
                "# Model evaluation\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
                "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# File handling\n",
                "import os\n",
                "import sys\n",
                "import warnings\n",
                "import joblib\n",
                "\n",
                "# Progress bar\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Ignore warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set plotting style\n",
                "sns.set(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "plt.rcParams['font.size'] = 12"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define paths\n",
                "DATA_PROCESSED_DIR = '../data/processed'\n",
                "RESULTS_DIR = '../results'\n",
                "FIGURES_DIR = os.path.join(RESULTS_DIR, 'figures')\n",
                "MODELS_DIR = os.path.join(RESULTS_DIR, 'models')\n",
                "\n",
                "# Create directories if they don't exist\n",
                "os.makedirs(DATA_PROCESSED_DIR, exist_ok=True)\n",
                "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
                "os.makedirs(MODELS_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Processed Data\n",
                "\n",
                "Load the processed data with extracted features from the previous notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load processed data\n",
                "# This will be created in the feature engineering notebook\n",
                "try:\n",
                "    X_train = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'X_train.csv'))\n",
                "    X_val = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'X_val.csv'))\n",
                "    X_test = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'X_test.csv'))\n",
                "    y_train = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'y_train.csv'))['label']\n",
                "    y_val = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'y_val.csv'))['label']\n",
                "    y_test = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'y_test.csv'))['label']\n",
                "    \n",
                "    print(f\"X_train shape: {X_train.shape}\")\n",
                "    print(f\"X_val shape: {X_val.shape}\")\n",
                "    print(f\"X_test shape: {X_test.shape}\")\n",
                "    print(f\"y_train shape: {y_train.shape}\")\n",
                "    print(f\"y_val shape: {y_val.shape}\")\n",
                "    print(f\"y_test shape: {y_test.shape}\")\n",
                "except FileNotFoundError:\n",
                "    print(\"Processed data files not found. Please run the feature engineering notebook first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Evaluation Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_model(model, X_train, X_val, y_train, y_val, model_name):\n",
                "    \"\"\"Evaluate a model and return performance metrics.\n",
                "    \n",
                "    Parameters\n",
                "    ----------\n",
                "    model : estimator\n",
                "        The model to evaluate\n",
                "    X_train : array-like\n",
                "        Training features\n",
                "    X_val : array-like\n",
                "        Validation features\n",
                "    y_train : array-like\n",
                "        Training labels\n",
                "    y_val : array-like\n",
                "        Validation labels\n",
                "    model_name : str\n",
                "        Name of the model for reporting\n",
                "    \n",
                "    Returns\n",
                "    -------\n",
                "    dict\n",
                "        Dictionary of performance metrics\n",
                "    \"\"\"\n",
                "    # Train the model\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Make predictions\n",
                "    y_train_pred = model.predict(X_train)\n",
                "    y_val_pred = model.predict(X_val)\n",
                "    \n",
                "    # Calculate probabilities if the model supports it\n",
                "    try:\n",
                "        y_train_prob = model.predict_proba(X_train)[:, 1]\n",
                "        y_val_prob = model.predict_proba(X_val)[:, 1]\n",
                "        train_auc = roc_auc_score(y_train, y_train_prob)\n",
                "        val_auc = roc_auc_score(y_val, y_val_prob)\n",
                "    except (AttributeError, IndexError):\n",
                "        train_auc = None\n",
                "        val_auc = None\n",
                "    \n",
                "    # Calculate metrics\n",
                "    metrics = {\n",
                "        'model_name': model_name,\n",
                "        'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
                "        'val_accuracy': accuracy_score(y_val, y_val_pred),\n",
                "        'train_precision': precision_score(y_train, y_train_pred),\n",
                "        'val_precision': precision_score(y_val, y_val_pred),\n",
                "        'train_recall': recall_score(y_train, y_train_pred),\n",
                "        'val_recall': recall_score(y_val, y_val_pred),\n",
                "        'train_f1': f1_score(y_train, y_train_pred),\n",
                "        'val_f1': f1_score(y_val, y_val_pred),\n",
                "        'train_auc': train_auc,\n",
                "        'val_auc': val_auc,\n",
                "        'train_confusion_matrix': confusion_matrix(y_train, y_train_pred),\n",
                "        'val_confusion_matrix': confusion_matrix(y_val, y_val_pred),\n",
                "        'model': model\n",
                "    }\n",
                "    \n",
                "    # Print metrics\n",
                "    print(f\"Model: {model_name}\")\n",
                "    print(f\"Train Accuracy: {metrics['train_accuracy']:.4f}\")\n",
                "    print(f\"Validation Accuracy: {metrics['val_accuracy']:.4f}\")\n",
                "    print(f\"Train F1 Score: {metrics['train_f1']:.4f}\")\n",
                "    print(f\"Validation F1 Score: {metrics['val_f1']:.4f}\")\n",
                "    if train_auc is not None:\n",
                "        print(f\"Train AUC: {metrics['train_auc']:.4f}\")\n",
                "        print(f\"Validation AUC: {metrics['val_auc']:.4f}\")\n",
                "    print(\"\\nValidation Confusion Matrix:\")\n",
                "    print(metrics['val_confusion_matrix'])\n",
                "    print(\"\\nClassification Report:\")\n",
                "    print(classification_report(y_val, y_val_pred))\n",
                "    \n",
                "    return metrics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Baseline Model: Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define and train logistic regression model\n",
                "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
                "lr_metrics = evaluate_model(lr_model, X_train, X_val, y_train, y_val, \"Logistic Regression\")\n",
                "\n",
                "# Save the model\n",
                "joblib.dump(lr_model, os.path.join(MODELS_DIR, 'logistic_regression.pkl'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. K-Nearest Neighbors (KNN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define and train KNN model\n",
                "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
                "knn_metrics = evaluate_model(knn_model, X_train, X_val, y_train, y_val, \"K-Nearest Neighbors\")\n",
                "\n",
                "# Save the model\n",
                "joblib.dump(knn_model, os.path.join(MODELS_DIR, 'knn.pkl'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define and train Random Forest model\n",
                "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "rf_metrics = evaluate_model(rf_model, X_train, X_val, y_train, y_val, \"Random Forest\")\n",
                "\n",
                "# Save the model\n",
                "joblib.dump(rf_model, os.path.join(MODELS_DIR, 'random_forest.pkl'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. XGBoost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define and train XGBoost model\n",
                "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
                "xgb_metrics = evaluate_model(xgb_model, X_train, X_val, y_train, y_val, \"XGBoost\")\n",
                "\n",
                "# Save the model\n",
                "joblib.dump(xgb_model, os.path.join(MODELS_DIR, 'xgboost.pkl'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Support Vector Machine (SVM)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define and train SVM model\n",
                "svm_model = SVC(probability=True, random_state=42)\n",
                "svm_metrics = evaluate_model(svm_model, X_train, X_val, y_train, y_val, \"Support Vector Machine\")\n",
                "\n",
                "# Save the model\n",
                "joblib.dump(svm_model, os.path.join(MODELS_DIR, 'svm.pkl'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Collect metrics for all models\n",
                "models = [lr_metrics, knn_metrics, rf_metrics, xgb_metrics, svm_metrics]\n",
                "model_names = [m['model_name'] for m in models]\n",
                "val_accuracy = [m['val_accuracy'] for m in models]\n",
                "val_precision = [m['val_precision'] for m in models]\n",
                "val_recall = [m['val_recall'] for m in models]\n",
                "val_f1 = [m['val_f1'] for m in models]\n",
                "val_auc = [m['val_auc'] for m in models]\n",
                "\n",
                "# Create a DataFrame for comparison\n",
                "comparison_df = pd.DataFrame({\n",
                "    'Model': model_names,\n",
                "    'Accuracy': val_accuracy,\n",
                "    'Precision': val_precision,\n",
                "    'Recall': val_recall,\n",
                "    'F1 Score': val_f1,\n",
                "    'AUC': val_auc\n",
                "})\n",
                "\n",
                "# Sort by F1 score\n",
                "comparison_df = comparison_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
                "\n",
                "# Display comparison\n",
                "print(\"Model Comparison:\")\n",
                "display(comparison_df)\n",
                "\n",
                "# Visualize comparison\n",
                "plt.figure(figsize=(12, 8))\n",
                "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\n",
                "comparison_df_plot = comparison_df.melt(id_vars='Model', value_vars=metrics_to_plot, var_name='Metric', value_name='Value')\n",
                "sns.barplot(data=comparison_df_plot, x='Model', y='Value', hue='Metric')\n",
                "plt.title('Model Comparison')\n",
                "plt.xlabel('Model')\n",
                "plt.ylabel('Score')\n",
                "plt.xticks(rotation=45)\n",
                "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, 'model_comparison.png'), dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. ROC Curve Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot ROC curves for all models\n",
                "plt.figure(figsize=(10, 8))\n",
                "\n",
                "for model_metrics in models:\n",
                "    model = model_metrics['model']\n",
                "    model_name = model_metrics['model_name']\n",
                "    \n",
                "    try:\n",
                "        y_val_prob = model.predict_proba(X_val)[:, 1]\n",
                "        fpr, tpr, _ = roc_curve(y_val, y_val_prob)\n",
                "        auc = roc_auc_score(y_val, y_val_prob)\n",
                "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')\n",
                "    except (AttributeError, IndexError):\n",
                "        print(f\"Could not calculate ROC curve for {model_name}\")\n",
                "\n",
                "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curve Comparison')\n",
                "plt.legend(loc='lower right')\n",
                "plt.grid(True)\n",
                "plt.savefig(os.path.join(FIGURES_DIR, 'roc_curve_comparison.png'), dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Feature Importance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze feature importance for models that support it\n",
                "feature_names = X_train.columns\n",
                "\n",
                "# Random Forest feature importance\n",
                "if hasattr(rf_model, 'feature_importances_'):\n",
                "    rf_importances = pd.DataFrame({\n",
                "        'Feature': feature_names,\n",
                "        'Importance': rf_model.feature_importances_\n",
                "    }).sort_values('Importance', ascending=False)\n",
                "    \n",
                "    # Plot top 20 features\n",
                "    plt.figure(figsize=(12, 8))\n",
                "    sns.barplot(data=rf_importances.head(20), x='Importance', y='Feature')\n",
                "    plt.title('Random Forest Feature Importance')\n",
                "    plt.xlabel('Importance')\n",
                "    plt.ylabel('Feature')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(os.path.join(FIGURES_DIR, 'rf_feature_importance.png'), dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "\n",
                "# XGBoost feature importance\n",
                "if hasattr(xgb_model, 'feature_importances_'):\n",
                "    xgb_importances = pd.DataFrame({\n",
                "        'Feature': feature_names,\n",
                "        'Importance': xgb_model.feature_importances_\n",
                "    }).sort_values('Importance', ascending=False)\n",
                "    \n",
                "    # Plot top 20 features\n",
                "    plt.figure(figsize=(12, 8))\n",
                "    sns.barplot(data=xgb_importances.head(20), x='Importance', y='Feature')\n",
                "    plt.title('XGBoost Feature Importance')\n",
                "    plt.xlabel('Importance')\n",
                "    plt.ylabel('Feature')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(os.path.join(FIGURES_DIR, 'xgb_feature_importance.png'), dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "\n",
                "# Logistic Regression coefficients\n",
                "if hasattr(lr_model, 'coef_'):\n",
                "    lr_coeffs = pd.DataFrame({\n",
                "        'Feature': feature_names,\n",
                "        'Coefficient': lr_model.coef_[0]\n",
                "    })\n",
                "    lr_coeffs['AbsCoefficient'] = np.abs(lr_coeffs['Coefficient'])\n",
                "    lr_coeffs = lr_coeffs.sort_values('AbsCoefficient', ascending=False)\n",
                "    \n",
                "    # Plot top 20 features\n",
                "    plt.figure(figsize=(12, 8))\n",
                "    sns.barplot(data=lr_coeffs.head(20), x='Coefficient', y='Feature')\n",
                "    plt.title('Logistic Regression Coefficients')\n",
                "    plt.xlabel('Coefficient')\n",
                "    plt.ylabel('Feature')\n",
                "    plt.axvline(x=0, color='r', linestyle='--')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(os.path.join(FIGURES_DIR, 'lr_coefficients.png'), dpi=300, bbox_inches='tight')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Hyperparameter Tuning for Best Model\n",
                "\n",
                "Based on the model comparison, we'll tune the hyperparameters of the best-performing model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify the best model from the comparison\n",
                "best_model_name = comparison_df.iloc[0]['Model']\n",
                "print(f\"Best model: {best_model_name}\")\n",
                "\n",
                "# Define hyperparameter grid based on the best model\n",
                "if best_model_name == \"Logistic Regression\":\n",
                "    model = LogisticRegression(random_state=42)\n",
                "    param_grid = {\n",
                "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
                "        'penalty': ['l1', 'l2'],\n",
                "        'solver': ['liblinear', 'saga']\n",
                "    }\n",
                "elif best_model_name == \"K-Nearest Neighbors\":\n",
                "    model = KNeighborsClassifier()\n",
                "    param_grid = {\n",
                "        'n_neighbors': [3, 5, 7, 9, 11],\n",
                "        'weights': ['uniform', 'distance'],\n",
                "        'metric': ['euclidean', 'manhattan', 'minkowski']\n",
                "    }\n",
                "elif best_model_name == \"Random Forest\":\n",
                "    model = RandomForestClassifier(random_state=42)\n",
                "    param_grid = {\n",
                "        'n_estimators': [50, 100, 200],\n",
                "        'max_depth': [None, 10, 20, 30],\n",
                "        'min_samples_split': [2, 5, 10],\n",
                "        'min_samples_leaf': [1, 2, 4]\n",
                "    }\n",
                "elif best_model_name == \"XGBoost\":\n",
                "    model = xgb.XGBClassifier(random_state=42)\n",
                "    param_grid = {\n",
                "        'n_estimators': [50, 100, 200],\n",
                "        'max_depth': [3, 5, 7],\n",
                "        'learning_rate': [0.01, 0.1, 0.2],\n",
                "        'subsample': [0.8, 0.9, 1.0],\n",
                "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
                "    }\n",
                "elif best_model_name == \"Support Vector Machine\":\n",
                "    model = SVC(probability=True, random_state=42)\n",
                "    param_grid = {\n",
                "        'C': [0.1, 1, 10, 100],\n",
                "        'gamma': ['scale', 'auto', 0.1, 0.01],\n",
                "        'kernel': ['rbf', 'linear', 'poly']\n",
                "    }\n",
                "else:\n",
                "    print(f\"No hyperparameter tuning defined for {best_model_name}\")\n",
                "    model = None\n",
                "    param_grid = {}\n",
                "\n",
                "# Perform grid search if a model and param_grid are defined\n",
                "if model is not None and param_grid:\n",
                "    print(f\"Performing grid search for {best_model_name}...\")\n",
                "    grid_search = GridSearchCV(\n",
                "        estimator=model,\n",
                "        param_grid=param_grid,\n",
                "        cv=5,\n",
                "        scoring='f1',\n",
                "        n_jobs=-1,\n",
                "        verbose=1\n",
                "    )\n",
                "    grid_search.fit(X_train, y_train)\n",
                "    \n",
                "    # Print best parameters and score\n",
                "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
                "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
                "    \n",
                "    # Evaluate the best model\n",
                "    best_model = grid_search.best_estimator_\n",
                "    best_model_metrics = evaluate_model(best_model, X_train, X_val, y_train, y_val, f\"Tuned {best_model_name}\")\n",
                "    \n",
                "    # Save the best model\n",
                "    joblib.dump(best_model, os.path.join(MODELS_DIR, f'tuned_{best_model_name.lower().replace(\" \", \"_\")}.pkl'))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 14. Final Model Evaluation on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the best model\n",
                "try:\n",
                "    best_model = joblib.load(os.path.join(MODELS_DIR, f'tuned_{best_model_name.lower().replace(\" \", \"_\")}.pkl'))\n",
                "except FileNotFoundError:\n",
                "    print(f\"Tuned model file not found. Using the original best model.\")\n",
                "    if best_model_name == \"Logistic Regression\":\n",
                "        best_model = lr_model\n",
                "    elif best_model_name == \"K-Nearest Neighbors\":\n",
                "        best_model = knn_model\n",
                "    elif best_model_name == \"Random Forest\":\n",
                "        best_model = rf_model\n",
                "    elif best_model_name == \"XGBoost\":\n",
                "        best_model = xgb_model\n",
                "    elif best_model_name == \"Support Vector Machine\":\n",
                "        best_model = svm_model\n",
                "\n",
                "# Evaluate on test set\n",
                "y_test_pred = best_model.predict(X_test)\n",
                "try:\n",
                "    y_test_prob = best_model.predict_proba(X_test)[:, 1]\n",
                "    test_auc = roc_auc_score(y_test, y_test_prob)\n",
                "except (AttributeError, IndexError):\n",
                "    test_auc = None\n",
                "\n",
                "# Calculate metrics\n",
                "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
                "test_precision = precision_score(y_test, y_test_pred)\n",
                "test_recall = recall_score(y_test, y_test_pred)\n",
                "test_f1 = f1_score(y_test, y_test_pred)\n",
                "test_cm = confusion_matrix(y_test, y_test_pred)\n",
                "\n",
                "# Print metrics\n",
                "print(f\"Final Model: {best_model_name}\")\n",
                "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
                "print(f\"Test Precision: {test_precision:.4f}\")\n",
                "print(f\"Test Recall: {test_recall:.4f}\")\n",
                "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
                "if test_auc is not None:\n",
                "    print(f\"Test AUC: {test_auc:.4f}\")\n",
                "print(\"\\nTest Confusion Matrix:\")\n",
                "print(test_cm)\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_test_pred))\n",
                "\n",
                "# Plot confusion matrix\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
                "            xticklabels=['Non-binder', 'Binder'],\n",
                "            yticklabels=['Non-binder', 'Binder'])\n",
                "plt.title(f'Confusion Matrix - {best_model_name} (Test Set)')\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, 'final_model_confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "# Plot ROC curve if applicable\n",
                "if test_auc is not None:\n",
                "    fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
                "    plt.figure(figsize=(8, 6))\n",
                "    plt.plot(fpr, tpr, label=f'AUC = {test_auc:.3f}')\n",
                "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
                "    plt.xlabel('False Positive Rate')\n",
                "    plt.ylabel('True Positive Rate')\n",
                "    plt.title(f'ROC Curve - {best_model_name} (Test Set)')\n",
                "    plt.legend(loc='lower right')\n",
                "    plt.grid(True)\n",
                "    plt.savefig(os.path.join(FIGURES_DIR, 'final_model_roc_curve.png'), dpi=300, bbox_inches='tight')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 15. Summary and Next Steps"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Summary\n",
                "\n",
                "In this notebook, we developed and evaluated several machine learning models for predicting antibody binding from amino acid sequence features:\n",
                "\n",
                "1. **Baseline Model**: Logistic Regression\n",
                "2. **Advanced Models**:\n",
                "   - K-Nearest Neighbors (KNN)\n",
                "   - Random Forest\n",
                "   - XGBoost\n",
                "   - Support Vector Machine (SVM)\n",
                "\n",
                "We compared these models using various performance metrics, including accuracy, precision, recall, F1 score, and AUC. The best-performing model was [Best Model], which achieved an F1 score of [Score] on the validation set and [Score] on the test set.\n",
                "\n",
                "We also analyzed feature importance to identify the most predictive features for antibody binding. The top features included [list top features].\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "1. **Further Model Optimization**:\n",
                "   - Explore ensemble methods combining multiple models\n",
                "   - Implement more advanced feature selection techniques\n",
                "   - Consider deep learning approaches for sequence data\n",
                "\n",
                "2. **Model Interpretation**:\n",
                "   - Analyze misclassified examples to understand model limitations\n",
                "   - Connect model predictions to biological mechanisms\n",
                "   - Investigate the relationship between important features and binding properties\n",
                "\n",
                "3. **Application**:\n",
                "   - Apply the model to new antibody sequences\n",
                "   - Develop a web interface or API for model deployment\n",
                "   - Integrate with existing antibody design workflows\n",
                "\n",
                "The next notebook will focus on detailed analysis of the results and biological interpretation of the model predictions."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}