{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Predicting Antibody Binding from Amino Acid Sequences\n",
                "\n",
                "## Data Preprocessing and Cleaning\n",
                "\n",
                "This notebook focuses on data preprocessing and cleaning steps for the antibody binding prediction project."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data processing and analysis\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import scipy.stats as stats\n",
                "\n",
                "# Bioinformatics\n",
                "from Bio import SeqIO\n",
                "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Utilities\n",
                "from tqdm.notebook import tqdm\n",
                "import os\n",
                "import sys\n",
                "import warnings\n",
                "\n",
                "# Set plotting style\n",
                "sns.set_style(\"whitegrid\")\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "\n",
                "# Ignore warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.max_rows', 100)\n",
                "pd.set_option('display.float_format', '{:.3f}'.format)\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define paths\n",
                "DATA_RAW_DIR = '../data/raw'\n",
                "DATA_PROCESSED_DIR = '../data/processed'\n",
                "RESULTS_DIR = '../results'\n",
                "FIGURES_DIR = '../results/figures'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Raw Data\n",
                "\n",
                "In this section, we'll load the AVIDa-SARS-CoV-2 dataset. The dataset contains 77,003 full-length VHH (alpaca) antibody sequences, including 22,002 binders and 55,001 non-binders across 12 SARS-CoV-2 variants."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_raw_data(filepath):\n",
                "    \"\"\"\n",
                "    Load the raw AVIDa-SARS-CoV-2 dataset.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    filepath : str\n",
                "        Path to the raw data file\n",
                "        \n",
                "    Returns:\n",
                "    --------\n",
                "    pd.DataFrame\n",
                "        DataFrame containing the raw data\n",
                "    \"\"\"\n",
                "    # This is a placeholder function that will be implemented when the actual data is available\n",
                "    # The actual implementation will depend on the format of the data\n",
                "    \n",
                "    print(f\"Loading data from {filepath}...\")\n",
                "    \n",
                "    # Example implementation (to be updated with actual data loading logic)\n",
                "    if not os.path.exists(filepath):\n",
                "        raise FileNotFoundError(f\"Data file not found at {filepath}\")\n",
                "    \n",
                "    # Assuming the data is in CSV format\n",
                "    try:\n",
                "        df = pd.read_csv(filepath)\n",
                "        print(f\"Successfully loaded data with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
                "        return df\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading data: {e}\")\n",
                "        raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Cleaning\n",
                "\n",
                "In this section, we'll clean the raw data by handling missing values, removing duplicates, and addressing any other data quality issues."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_amino_acid_sequences(df, sequence_col='amino_acid_sequence'):\n",
                "    \"\"\"\n",
                "    Clean and validate amino acid sequences.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    df : pd.DataFrame\n",
                "        DataFrame containing the raw data\n",
                "    sequence_col : str, optional\n",
                "        Name of the column containing amino acid sequences\n",
                "        \n",
                "    Returns:\n",
                "    --------\n",
                "    pd.DataFrame\n",
                "        DataFrame with cleaned sequences\n",
                "    \"\"\"\n",
                "    print(\"Cleaning amino acid sequences...\")\n",
                "    \n",
                "    # Make a copy of the dataframe to avoid modifying the original\n",
                "    df_clean = df.copy()\n",
                "    \n",
                "    # Check for missing sequences\n",
                "    missing_seq = df_clean[sequence_col].isna().sum()\n",
                "    print(f\"Found {missing_seq} missing sequences\")\n",
                "    \n",
                "    # Remove rows with missing sequences\n",
                "    df_clean = df_clean.dropna(subset=[sequence_col])\n",
                "    \n",
                "    # Check for invalid amino acid codes\n",
                "    # Valid amino acid codes are: A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y\n",
                "    valid_aa = set('ACDEFGHIKLMNPQRSTVWY')\n",
                "    \n",
                "    # Function to check if a sequence contains only valid amino acids\n",
                "    def is_valid_sequence(seq):\n",
                "        return set(seq.upper()).issubset(valid_aa)\n",
                "    \n",
                "    # Check for invalid sequences\n",
                "    invalid_seq = df_clean[~df_clean[sequence_col].apply(is_valid_sequence)]\n",
                "    print(f\"Found {len(invalid_seq)} sequences with invalid amino acid codes\")\n",
                "    \n",
                "    # Remove rows with invalid sequences\n",
                "    df_clean = df_clean[df_clean[sequence_col].apply(is_valid_sequence)]\n",
                "    \n",
                "    # Convert sequences to uppercase\n",
                "    df_clean[sequence_col] = df_clean[sequence_col].str.upper()\n",
                "    \n",
                "    print(f\"Cleaning complete. Final dataset has {len(df_clean)} rows\")\n",
                "    return df_clean"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Handle Class Imbalance\n",
                "\n",
                "The dataset has a class imbalance with 22,002 binders and 55,001 non-binders. We'll address this imbalance to ensure our models are not biased."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def handle_class_imbalance(df, target_col='binding_label', method='class_weight'):\n",
                "    \"\"\"\n",
                "    Handle class imbalance in the dataset.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    df : pd.DataFrame\n",
                "        DataFrame containing the data\n",
                "    target_col : str, optional\n",
                "        Name of the column containing the target variable\n",
                "    method : str, optional\n",
                "        Method to handle class imbalance ('class_weight', 'undersampling', 'oversampling', 'smote')\n",
                "        \n",
                "    Returns:\n",
                "    --------\n",
                "    pd.DataFrame or tuple\n",
                "        Depending on the method, either the modified DataFrame or a tuple containing the DataFrame and class weights\n",
                "    \"\"\"\n",
                "    print(f\"Handling class imbalance using method: {method}\")\n",
                "    \n",
                "    # Check class distribution\n",
                "    class_counts = df[target_col].value_counts()\n",
                "    print(f\"Class distribution:\\n{class_counts}\")\n",
                "    \n",
                "    if method == 'class_weight':\n",
                "        # Calculate class weights\n",
                "        n_samples = len(df)\n",
                "        n_classes = len(class_counts)\n",
                "        class_weights = {i: n_samples / (n_classes * count) for i, count in enumerate(class_counts)}\n",
                "        print(f\"Class weights: {class_weights}\")\n",
                "        return df, class_weights\n",
                "    \n",
                "    elif method == 'undersampling':\n",
                "        # Undersample the majority class\n",
                "        from sklearn.utils import resample\n",
                "        \n",
                "        # Separate majority and minority classes\n",
                "        df_majority = df[df[target_col] == class_counts.idxmax()]\n",
                "        df_minority = df[df[target_col] == class_counts.idxmin()]\n",
                "        \n",
                "        # Undersample majority class\n",
                "        df_majority_undersampled = resample(\n",
                "            df_majority, \n",
                "            replace=False,\n",
                "            n_samples=len(df_minority),\n",
                "            random_state=42\n",
                "        )\n",
                "        \n",
                "        # Combine minority class with undersampled majority class\n",
                "        df_balanced = pd.concat([df_majority_undersampled, df_minority])\n",
                "        \n",
                "        print(f\"After undersampling, class distribution:\\n{df_balanced[target_col].value_counts()}\")\n",
                "        return df_balanced\n",
                "    \n",
                "    elif method == 'oversampling':\n",
                "        # Oversample the minority class\n",
                "        from sklearn.utils import resample\n",
                "        \n",
                "        # Separate majority and minority classes\n",
                "        df_majority = df[df[target_col] == class_counts.idxmax()]\n",
                "        df_minority = df[df[target_col] == class_counts.idxmin()]\n",
                "        \n",
                "        # Oversample minority class\n",
                "        df_minority_oversampled = resample(\n",
                "            df_minority, \n",
                "            replace=True,\n",
                "            n_samples=len(df_majority),\n",
                "            random_state=42\n",
                "        )\n",
                "        \n",
                "        # Combine oversampled minority class with majority class\n",
                "        df_balanced = pd.concat([df_majority, df_minority_oversampled])\n",
                "        \n",
                "        print(f\"After oversampling, class distribution:\\n{df_balanced[target_col].value_counts()}\")\n",
                "        return df_balanced\n",
                "    \n",
                "    elif method == 'smote':\n",
                "        # Use SMOTE to generate synthetic samples\n",
                "        try:\n",
                "            from imblearn.over_sampling import SMOTE\n",
                "            \n",
                "            # Assuming X contains features and y contains target\n",
                "            X = df.drop(columns=[target_col])\n",
                "            y = df[target_col]\n",
                "            \n",
                "            # Apply SMOTE\n",
                "            smote = SMOTE(random_state=42)\n",
                "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
                "            \n",
                "            # Create a new DataFrame with balanced classes\n",
                "            df_balanced = pd.DataFrame(X_resampled, columns=X.columns)\n",
                "            df_balanced[target_col] = y_resampled\n",
                "            \n",
                "            print(f\"After SMOTE, class distribution:\\n{df_balanced[target_col].value_counts()}\")\n",
                "            return df_balanced\n",
                "        except ImportError:\n",
                "            print(\"SMOTE requires the imbalanced-learn package. Please install it using 'pip install imbalanced-learn'\")\n",
                "            return df\n",
                "    \n",
                "    else:\n",
                "        print(f\"Unknown method: {method}. Returning original DataFrame.\")\n",
                "        return df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Split Data\n",
                "\n",
                "We'll split the data into training, validation, and test sets while ensuring proper stratification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "def split_data(df, target_col='binding_label', test_size=0.2, val_size=0.2, random_state=42):\n",
                "    \"\"\"\n",
                "    Split the data into training, validation, and test sets.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    df : pd.DataFrame\n",
                "        DataFrame containing the preprocessed data\n",
                "    target_col : str, optional\n",
                "        Name of the column containing the target variable\n",
                "    test_size : float, optional\n",
                "        Proportion of data to use for testing\n",
                "    val_size : float, optional\n",
                "        Proportion of training data to use for validation\n",
                "    random_state : int, optional\n",
                "        Random seed for reproducibility\n",
                "        \n",
                "    Returns:\n",
                "    --------\n",
                "    tuple\n",
                "        (X_train, X_val, X_test, y_train, y_val, y_test)\n",
                "    \"\"\"\n",
                "    from sklearn.model_selection import train_test_split\n",
                "    \n",
                "    # Separate features and target\n",
                "    X = df.drop(columns=[target_col])\n",
                "    y = df[target_col]\n",
                "    \n",
                "    # First, split into training+validation and test\n",
                "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
                "        X, y, \n",
                "        test_size=test_size, \n",
                "        random_state=random_state,\n",
                "        stratify=y  # Stratify by the target variable\n",
                "    )\n",
                "    \n",
                "    # Then split training+validation into training and validation\n",
                "    # Adjust validation size to account for the reduced dataset size\n",
                "    adjusted_val_size = val_size / (1 - test_size)\n",
                "    \n",
                "    X_train, X_val, y_train, y_val = train_test_split(\n",
                "        X_train_val, y_train_val, \n",
                "        test_size=adjusted_val_size, \n",
                "        random_state=random_state,\n",
                "        stratify=y_train_val  # Stratify by the target variable\n",
                "    )\n",
                "    \n",
                "    print(f\"Data split complete:\")\n",
                "    print(f\"  Training set: {X_train.shape[0]} samples\")\n",
                "    print(f\"  Validation set: {X_val.shape[0]} samples\")\n",
                "    print(f\"  Test set: {X_test.shape[0]} samples\")\n",
                "    \n",
                "    return X_train, X_val, X_test, y_train, y_val, y_test"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Processed Data\n",
                "\n",
                "We'll save the processed data for use in subsequent notebooks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_processed_data(X_train, X_val, X_test, y_train, y_val, y_test, output_dir=DATA_PROCESSED_DIR):\n",
                "    \"\"\"\n",
                "    Save the processed data to disk.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    X_train, X_val, X_test : pd.DataFrame\n",
                "        Feature matrices for training, validation, and test sets\n",
                "    y_train, y_val, y_test : pd.Series\n",
                "        Target variables for training, validation, and test sets\n",
                "    output_dir : str, optional\n",
                "        Directory to save the processed data\n",
                "    \"\"\"\n",
                "    # Create the output directory if it doesn't exist\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    \n",
                "    # Save the data\n",
                "    X_train.to_csv(os.path.join(output_dir, 'X_train.csv'), index=False)\n",
                "    X_val.to_csv(os.path.join(output_dir, 'X_val.csv'), index=False)\n",
                "    X_test.to_csv(os.path.join(output_dir, 'X_test.csv'), index=False)\n",
                "    y_train.to_csv(os.path.join(output_dir, 'y_train.csv'), index=False)\n",
                "    y_val.to_csv(os.path.join(output_dir, 'y_val.csv'), index=False)\n",
                "    y_test.to_csv(os.path.join(output_dir, 'y_test.csv'), index=False)\n",
                "    \n",
                "    print(f\"Processed data saved to {output_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Next Steps\n",
                "\n",
                "The data preprocessing and cleaning steps are now defined. In the next notebook, we'll focus on feature engineering from amino acid sequences."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
